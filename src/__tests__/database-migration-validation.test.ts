/**
 * Database Migration Validation Tests
 * 
 * Validates Phase 2 database migration that corrected 4,313+ records
 * Tests the data recovery service and migration integrity
 */

import { describe, test, expect, beforeEach, afterEach } from 'vitest'
import Database from 'better-sqlite3'
import { DataRecoveryService } from '../database/services/data-recovery-service'
import { BreadthService } from '../database/services/breadth-service'
import path from 'path'
import fs from 'fs'

const TEST_DB_PATH = path.join(__dirname, 'test-migration-validation.db')

describe('Database Migration Validation', () => {
  let db: Database.Database
  let recoveryService: DataRecoveryService
  let breadthService: BreadthService

  beforeEach(() => {
    // Clean up any existing test database
    if (fs.existsSync(TEST_DB_PATH)) {
      fs.unlinkSync(TEST_DB_PATH)
    }

    // Create test database with schema
    db = new Database(TEST_DB_PATH)
    
    // Create market_breadth table with full schema
    db.exec(`
      CREATE TABLE IF NOT EXISTS market_breadth (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        date TEXT NOT NULL,
        timestamp TEXT,
        advancing_issues INTEGER,
        declining_issues INTEGER,
        new_highs INTEGER,
        new_lows INTEGER,
        up_volume REAL,
        down_volume REAL,
        breadth_score REAL,
        trend_strength REAL,
        market_phase TEXT,
        data_source TEXT DEFAULT 'manual',
        notes TEXT,
        
        -- Enhanced indicators
        stocks_up_4pct INTEGER,
        stocks_down_4pct INTEGER,
        stocks_up_20pct INTEGER,
        stocks_down_20pct INTEGER,
        stocks_up_20dollar INTEGER,
        stocks_down_20dollar INTEGER,
        ratio_5day REAL,
        ratio_10day REAL,
        stocks_up_25pct_quarter INTEGER,
        stocks_down_25pct_quarter INTEGER,
        stocks_up_25pct_month INTEGER,
        stocks_down_25pct_month INTEGER,
        stocks_up_50pct_month INTEGER,
        stocks_down_50pct_month INTEGER,
        stocks_up_13pct_34days INTEGER,
        stocks_down_13pct_34days INTEGER,
        worden_universe INTEGER DEFAULT 7000,
        t2108 REAL,
        sp500 TEXT,
        vix REAL,
        source_file TEXT,
        import_format TEXT,
        data_quality_score INTEGER DEFAULT 100,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
        
        UNIQUE(date)
      )
    `)

    // Initialize services
    recoveryService = new DataRecoveryService(TEST_DB_PATH)
    breadthService = new BreadthService(db)
  })

  afterEach(() => {
    recoveryService.close()
    db.close()
    if (fs.existsSync(TEST_DB_PATH)) {
      fs.unlinkSync(TEST_DB_PATH)
    }
  })

  describe('Pre-Migration Data Corruption Scenarios', () => {
    test('should identify S&P 500 multi-value corruption patterns', () => {
      console.log('ðŸ§ª Testing S&P 500 corruption identification...')
      
      // Insert corrupted records similar to pre-migration state
      const corruptedRecords = [
        { date: '2024-01-15', sp500: '5825.15 6120.45', notes: 'CSV: up4%=175' },
        { date: '2024-01-16', sp500: '5,832.92', notes: 'CSV: up4%=180' },
        { date: '2024-01-17', sp500: '5810.22, 6150.30', notes: 'CSV: up4%=165' },
        { date: '2024-01-18', sp500: '85.25', notes: 'CSV: up4%=190' }, // Clearly corrupted low value
        { date: '2024-01-19', sp500: 'ABC 5840.50', notes: 'CSV: up4%=185' } // Text corruption
      ]
      
      for (const record of corruptedRecords) {
        db.prepare(`
          INSERT INTO market_breadth (date, sp500, notes) 
          VALUES (?, ?, ?)
        `).run(record.date, record.sp500, record.notes)
      }
      
      // Analyze corruption
      const corruptionAnalysis = recoveryService.analyzeCorruption()
      
      expect(corruptionAnalysis.sp500Corruption.totalCorrupted).toBe(5)
      expect(corruptionAnalysis.sp500Corruption.multiValuePattern).toBeGreaterThan(0)
      expect(corruptionAnalysis.sp500Corruption.commaFormatting).toBeGreaterThan(0)
      expect(corruptionAnalysis.sp500Corruption.invalidValues).toBeGreaterThan(0)
      
      console.log('ðŸ“Š Corruption Analysis:', corruptionAnalysis.sp500Corruption)
      console.log('âœ… S&P 500 corruption patterns identified correctly')
    })

    test('should identify missing secondary indicator patterns', () => {
      console.log('ðŸ§ª Testing secondary indicator corruption identification...')
      
      // Insert records with missing secondary indicators
      const incompleteRecords = [
        { 
          date: '2024-02-15', 
          stocks_up_4pct: 175, 
          stocks_down_4pct: 125, 
          notes: 'CSV: up4%=175, down4%=125'
        },
        { 
          date: '2024-02-16', 
          stocks_up_4pct: 180, 
          stocks_down_4pct: 115,
          stocks_up_25pct_quarter: null,
          stocks_down_25pct_quarter: null,
          notes: 'CSV: up4%=180, down4%=115, up25q%=, down25q%='
        },
        { 
          date: '2024-02-17', 
          advancing_issues: 1850, // Legacy field with data
          declining_issues: 1150,
          notes: 'Old format data'
        }
      ]
      
      for (const record of incompleteRecords) {
        const stmt = db.prepare(`
          INSERT INTO market_breadth (date, stocks_up_4pct, stocks_down_4pct, 
                                    stocks_up_25pct_quarter, stocks_down_25pct_quarter,
                                    advancing_issues, declining_issues, notes) 
          VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        `)
        stmt.run(
          record.date, 
          record.stocks_up_4pct || null,
          record.stocks_down_4pct || null,
          record.stocks_up_25pct_quarter || null,
          record.stocks_down_25pct_quarter || null,
          record.advancing_issues || null,
          record.declining_issues || null,
          record.notes
        )
      }
      
      const corruptionAnalysis = recoveryService.analyzeCorruption()
      
      expect(corruptionAnalysis.secondaryIndicators.totalMissing).toBeGreaterThan(0)
      expect(corruptionAnalysis.secondaryIndicators.quarterlyFields).toBeGreaterThan(0)
      
      console.log('ðŸ“Š Secondary Indicators Analysis:', corruptionAnalysis.secondaryIndicators)
      console.log('âœ… Secondary indicator missing patterns identified correctly')
    })
  })

  describe('Migration Recovery Operations', () => {
    test('should recover S&P 500 multi-value corruption', () => {
      console.log('ðŸ§ª Testing S&P 500 multi-value recovery...')
      
      // Insert corrupted S&P data
      const corruptedData = [
        { date: '2024-03-15', sp500: '5825.15 6120.45' },
        { date: '2024-03-16', sp500: '5,832.92' },
        { date: '2024-03-17', sp500: '5810.22, 6150.30' }
      ]
      
      for (const record of corruptedData) {
        db.prepare('INSERT INTO market_breadth (date, sp500) VALUES (?, ?)')
          .run(record.date, record.sp500)
      }
      
      // Execute recovery
      const recoveryResult = breadthService.recoverCorruptedSP500Values()
      
      expect(recoveryResult.recovered).toBe(3)
      expect(recoveryResult.failed).toBe(0)
      
      // Verify corrections
      const correctedRecords = db.prepare(`
        SELECT date, sp500 FROM market_breadth 
        WHERE date BETWEEN '2024-03-15' AND '2024-03-17'
        ORDER BY date
      `).all()
      
      // All S&P values should be single numeric values > 5000
      for (const record of correctedRecords) {
        const sp500Value = parseFloat(record.sp500)
        expect(sp500Value).toBeGreaterThan(5000)
        expect(sp500Value).toBeLessThan(10000)
        expect(record.sp500).not.toMatch(/\s/) // No spaces
        expect(record.sp500).not.toMatch(/,/) // No commas in final value
      }
      
      console.log('âœ… S&P 500 multi-value recovery successful:', {
        recovered: recoveryResult.recovered,
        correctedRecords: correctedRecords.map(r => ({ date: r.date, sp500: r.sp500 }))
      })
    })

    test('should recover secondary indicators using correlation', () => {
      console.log('ðŸ§ª Testing secondary indicator correlation recovery...')
      
      // Insert records with missing secondary indicators but available primary data
      const incompleteRecords = [
        {
          date: '2024-04-15',
          stocks_up_4pct: 175,
          stocks_down_4pct: 125,
          t2108: 65 // High T2108 should correlate with higher quarterly moves
        },
        {
          date: '2024-04-16', 
          stocks_up_4pct: 180,
          stocks_down_4pct: 115,
          t2108: 35 // Low T2108 should correlate with higher declines
        },
        {
          date: '2024-04-17',
          new_highs: 850, // Legacy field with realistic quarterly range
          new_lows: 420,  // Legacy field with realistic quarterly range
          stocks_up_4pct: 165,
          stocks_down_4pct: 135
        }
      ]
      
      for (const record of incompleteRecords) {
        const stmt = db.prepare(`
          INSERT INTO market_breadth (
            date, stocks_up_4pct, stocks_down_4pct, t2108, new_highs, new_lows
          ) VALUES (?, ?, ?, ?, ?, ?)
        `)
        stmt.run(
          record.date,
          record.stocks_up_4pct || null,
          record.stocks_down_4pct || null,
          record.t2108 || null,
          record.new_highs || null,
          record.new_lows || null
        )
      }
      
      // Execute secondary indicator recovery
      const recoveryResult = breadthService.recoverSecondaryIndicators()
      
      expect(recoveryResult.recovered).toBeGreaterThan(0)
      
      // Verify correlations were applied reasonably
      const recoveredRecords = db.prepare(`
        SELECT * FROM market_breadth 
        WHERE date BETWEEN '2024-04-15' AND '2024-04-17'
        ORDER BY date
      `).all()
      
      for (const record of recoveredRecords) {
        // Check if any quarterly fields were populated
        if (record.stocks_up_25pct_quarter || record.stocks_down_25pct_quarter) {
          expect(record.stocks_up_25pct_quarter).toBeGreaterThan(0)
          expect(record.stocks_down_25pct_quarter).toBeGreaterThan(0)
        }
      }
      
      console.log('âœ… Secondary indicator correlation recovery:', {
        recovered: recoveryResult.recovered,
        recordsWithQuarterly: recoveredRecords.filter(r => r.stocks_up_25pct_quarter).length
      })
    })

    test('should execute complete recovery process', async () => {
      console.log('ðŸ§ª Testing complete recovery process...')
      
      // Create a mixed dataset with various corruption types
      const mixedCorruptionData = [
        // S&P corruption
        { date: '2024-05-15', sp500: '5825.15 6120.45', notes: 'CSV: up4%=175, down4%=125' },
        { date: '2024-05-16', sp500: '5,832.92', notes: 'CSV: up4%=180, down4%=115' },
        
        // Missing secondary indicators
        { date: '2024-05-17', sp500: '5840.75', stocks_up_4pct: 165, stocks_down_4pct: 135 },
        { date: '2024-05-18', sp500: '5835.50', new_highs: 750, new_lows: 380, t2108: 68 },
        
        // Notes extraction scenarios
        { 
          date: '2024-05-19', 
          sp500: '5845.25', 
          notes: 'RECOVERED: up4%=185, down4%=120, up25q%=450, down25q%=200, T2108=66' 
        }
      ]
      
      for (const record of mixedCorruptionData) {
        const stmt = db.prepare(`
          INSERT INTO market_breadth (
            date, sp500, stocks_up_4pct, stocks_down_4pct, new_highs, new_lows, t2108, notes
          ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        `)
        stmt.run(
          record.date,
          record.sp500 || null,
          record.stocks_up_4pct || null,
          record.stocks_down_4pct || null,
          record.new_highs || null,
          record.new_lows || null,
          record.t2108 || null,
          record.notes || null
        )
      }
      
      // Execute complete recovery
      const completeRecoveryResult = await recoveryService.executeCompleteRecovery()
      
      expect(completeRecoveryResult.success).toBe(true)
      expect(completeRecoveryResult.totalRecordsProcessed).toBe(5)
      expect(completeRecoveryResult.sp500Recovery.recovered).toBeGreaterThan(0)
      
      // Verify final state
      const verificationResult = breadthService.verifyRecoverySuccess()
      
      expect(verificationResult.corruptedSP500).toBe(0) // Should be 0 after recovery
      expect(verificationResult.successRate).toBeGreaterThan(90) // Should be >90% success
      
      console.log('âœ… Complete recovery process:', {
        totalProcessed: completeRecoveryResult.totalRecordsProcessed,
        sp500Recovered: completeRecoveryResult.sp500Recovery.recovered,
        secondaryRecovered: completeRecoveryResult.secondaryIndicators.recovered,
        finalSuccessRate: verificationResult.successRate
      })
    })
  })

  describe('Migration Integrity Validation', () => {
    test('should validate data integrity after migration', () => {
      console.log('ðŸ§ª Testing data integrity validation...')
      
      // Insert clean data (post-migration state)
      const cleanData = [
        {
          date: '2024-06-15',
          stocks_up_4pct: 180,
          stocks_down_4pct: 120,
          stocks_up_25pct_quarter: 450,
          stocks_down_25pct_quarter: 200,
          t2108: 65.5,
          sp500: '5847.25',
          ratio_5day: 1.50,
          ratio_10day: 1.65
        },
        {
          date: '2024-06-16', 
          stocks_up_4pct: 175,
          stocks_down_4pct: 125,
          stocks_up_25pct_quarter: 430,
          stocks_down_25pct_quarter: 220,
          t2108: 63.2,
          sp500: '5835.80',
          ratio_5day: 1.40,
          ratio_10day: 1.54
        }
      ]
      
      for (const record of cleanData) {
        const stmt = db.prepare(`
          INSERT INTO market_breadth (
            date, stocks_up_4pct, stocks_down_4pct, stocks_up_25pct_quarter, 
            stocks_down_25pct_quarter, t2108, sp500, ratio_5day, ratio_10day
          ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        `)
        stmt.run(
          record.date, record.stocks_up_4pct, record.stocks_down_4pct,
          record.stocks_up_25pct_quarter, record.stocks_down_25pct_quarter,
          record.t2108, record.sp500, record.ratio_5day, record.ratio_10day
        )
      }
      
      // Validate integrity
      const integrityCheck = db.prepare(`
        SELECT 
          COUNT(*) as total_records,
          COUNT(stocks_up_4pct) as has_up_4pct,
          COUNT(stocks_down_4pct) as has_down_4pct,
          COUNT(stocks_up_25pct_quarter) as has_up_25q,
          COUNT(stocks_down_25pct_quarter) as has_down_25q,
          COUNT(t2108) as has_t2108,
          COUNT(sp500) as has_sp500,
          AVG(CASE WHEN sp500 NOT LIKE '% %' AND sp500 NOT LIKE '%,%' THEN 1 ELSE 0 END) as sp500_clean_rate,
          AVG(CASE WHEN t2108 BETWEEN 0 AND 100 THEN 1 ELSE 0 END) as t2108_valid_rate
        FROM market_breadth
      `).get() as any
      
      expect(integrityCheck.total_records).toBe(2)
      expect(integrityCheck.has_up_4pct).toBe(2)
      expect(integrityCheck.has_down_4pct).toBe(2) 
      expect(integrityCheck.has_up_25q).toBe(2)
      expect(integrityCheck.has_t2108).toBe(2)
      expect(integrityCheck.has_sp500).toBe(2)
      expect(integrityCheck.sp500_clean_rate).toBe(1) // 100% clean
      expect(integrityCheck.t2108_valid_rate).toBe(1) // 100% valid
      
      console.log('âœ… Data integrity validation passed:', integrityCheck)
    })

    test('should verify field population rates match expected migration results', () => {
      console.log('ðŸ§ª Testing field population rate validation...')
      
      // Simulate post-migration dataset with expected population rates
      const postMigrationData = Array.from({ length: 100 }, (_, i) => {
        const date = new Date('2024-01-01')
        date.setDate(date.getDate() + i)
        const dateStr = date.toISOString().split('T')[0]
        
        return {
          date: dateStr,
          stocks_up_4pct: Math.floor(Math.random() * 200) + 100,
          stocks_down_4pct: Math.floor(Math.random() * 150) + 80,
          
          // 95% should have quarterly data (expected migration success rate)
          stocks_up_25pct_quarter: i < 95 ? Math.floor(Math.random() * 500) + 300 : null,
          stocks_down_25pct_quarter: i < 95 ? Math.floor(Math.random() * 300) + 150 : null,
          
          // 90% should have T2108 (expected migration success rate)
          t2108: i < 90 ? Math.random() * 60 + 20 : null,
          
          // 98% should have clean S&P values (expected migration success rate)
          sp500: i < 98 ? (Math.random() * 1000 + 5000).toFixed(2) : null
        }
      })
      
      for (const record of postMigrationData) {
        const stmt = db.prepare(`
          INSERT INTO market_breadth (
            date, stocks_up_4pct, stocks_down_4pct, stocks_up_25pct_quarter,
            stocks_down_25pct_quarter, t2108, sp500
          ) VALUES (?, ?, ?, ?, ?, ?, ?)
        `)
        stmt.run(
          record.date, record.stocks_up_4pct, record.stocks_down_4pct,
          record.stocks_up_25pct_quarter, record.stocks_down_25pct_quarter,
          record.t2108, record.sp500
        )
      }
      
      // Calculate population rates
      const populationStats = db.prepare(`
        SELECT 
          COUNT(*) as total,
          COUNT(stocks_up_4pct) * 100.0 / COUNT(*) as up_4pct_rate,
          COUNT(stocks_down_4pct) * 100.0 / COUNT(*) as down_4pct_rate,
          COUNT(stocks_up_25pct_quarter) * 100.0 / COUNT(*) as up_25q_rate,
          COUNT(stocks_down_25pct_quarter) * 100.0 / COUNT(*) as down_25q_rate,
          COUNT(t2108) * 100.0 / COUNT(*) as t2108_rate,
          COUNT(sp500) * 100.0 / COUNT(*) as sp500_rate
        FROM market_breadth
      `).get() as any
      
      // Verify expected population rates
      expect(populationStats.total).toBe(100)
      expect(populationStats.up_4pct_rate).toBe(100) // Should be 100%
      expect(populationStats.down_4pct_rate).toBe(100) // Should be 100%
      expect(populationStats.up_25q_rate).toBeCloseTo(95, 1) // Should be ~95%
      expect(populationStats.down_25q_rate).toBeCloseTo(95, 1) // Should be ~95%
      expect(populationStats.t2108_rate).toBeCloseTo(90, 1) // Should be ~90%
      expect(populationStats.sp500_rate).toBeCloseTo(98, 1) // Should be ~98%
      
      console.log('âœ… Field population rates match migration expectations:', populationStats)
    })
  })

  describe('Performance Impact Validation', () => {
    test('should verify migration did not impact query performance', () => {
      console.log('ðŸ§ª Testing query performance after migration...')
      
      // Insert substantial dataset
      const largeDataset = Array.from({ length: 1000 }, (_, i) => {
        const date = new Date('2023-01-01')
        date.setDate(date.getDate() + i)
        return {
          date: date.toISOString().split('T')[0],
          stocks_up_4pct: Math.floor(Math.random() * 200) + 100,
          stocks_down_4pct: Math.floor(Math.random() * 150) + 80,
          stocks_up_25pct_quarter: Math.floor(Math.random() * 500) + 300,
          t2108: Math.random() * 60 + 20,
          sp500: (Math.random() * 1000 + 5000).toFixed(2)
        }
      })
      
      // Batch insert for performance
      const insertStmt = db.prepare(`
        INSERT INTO market_breadth (
          date, stocks_up_4pct, stocks_down_4pct, stocks_up_25pct_quarter, t2108, sp500
        ) VALUES (?, ?, ?, ?, ?, ?)
      `)
      
      const insertTransaction = db.transaction((records: any[]) => {
        for (const record of records) {
          insertStmt.run(record.date, record.stocks_up_4pct, record.stocks_down_4pct,
                        record.stocks_up_25pct_quarter, record.t2108, record.sp500)
        }
      })
      
      insertTransaction(largeDataset)
      
      // Test query performance
      const queries = [
        'SELECT * FROM market_breadth ORDER BY date DESC LIMIT 30',
        'SELECT * FROM market_breadth WHERE date BETWEEN "2023-06-01" AND "2023-07-01"',
        'SELECT AVG(stocks_up_4pct), AVG(t2108) FROM market_breadth WHERE t2108 IS NOT NULL',
        'SELECT COUNT(*) FROM market_breadth WHERE stocks_up_25pct_quarter > 400'
      ]
      
      for (const query of queries) {
        const start = performance.now()
        const result = db.prepare(query).all()
        const end = performance.now()
        
        expect(result.length).toBeGreaterThan(0)
        expect(end - start).toBeLessThan(100) // Should execute in <100ms
      }
      
      console.log('âœ… Query performance maintained after migration')
    })
  })
})